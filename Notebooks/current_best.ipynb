{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"../train_data.csv\")\n",
    "n_original = df_original.shape[0]\n",
    "df_submit = pd.read_csv(\"../sample_submission.csv\")\n",
    "df = pd.concat([df_original, df_submit], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Features from GRU\n",
    "df = pd.read_csv(\"../GRU_features_predict_only.csv\", index_col=0).merge(\n",
    "    df,\n",
    "    on='id'\n",
    ")\n",
    "\n",
    "# Features from other pretrained model\n",
    "df = pd.read_csv(\"../pretrained_feature_predict.csv\", index_col=0).merge(\n",
    "    df,\n",
    "    on='id'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siRNA_feat_builder(s: pd.Series, anti: bool = False):\n",
    "    name = \"anti\" if anti else \"sense\"\n",
    "    df = s.to_frame()\n",
    "    df[f\"feat_siRNA_{name}_seq_len\"] = s.str.len()\n",
    "    for pos in [0, -1]:\n",
    "        for c in list(\"AUGC\"):\n",
    "            df[f\"feat_siRNA_{name}_seq_{c}_{'front' if pos == 0 else 'back'}\"] = (\n",
    "                s.str[pos] == c\n",
    "            )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_1\"] = s.str.startswith(\"AA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_2\"] = s.str.startswith(\"GA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_3\"] = s.str.startswith(\"CA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_4\"] = s.str.startswith(\"UA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_5\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"AA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_6\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"GA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_7\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"CA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_8\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"UA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_9\"] = s.str[1] == \"A\"\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_10\"] = s.str[-2] == \"A\"\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_GC_ratio_0\"] = (\n",
    "        s.str.count(\"G\") + s.str.count(\"C\")\n",
    "    ) / s.str.len()\n",
    "\n",
    "    df[f\"feat_siRNA_{name}_len_range\"] = (s.str.len() >= 21) & (s.str.len() <= 25)\n",
    "\n",
    "    GC_ratio_1 = (s.str.count(\"G\") + s.str.count(\"C\")) / s.str.len()\n",
    "    df[f\"feat_siRNA_{name}_GC_ratio_1\"] = (GC_ratio_1 >= 0.31) & (GC_ratio_1 <= 0.58)\n",
    "\n",
    "    GC_ratio_2 = (s.str[1:7].str.count(\"G\") + s.str[1:7].str.count(\"C\")) / s.str[1:7].str.len()\n",
    "    df[f\"feat_siRNA_{name}_GC_ratio_2\"] = (GC_ratio_2 == 0.19)\n",
    "\n",
    "    GC_ratio_3 = (s.str[7:18].str.count(\"G\") + s.str[7:18].str.count(\"C\")) / s.str[7:18].str.len()\n",
    "    df[f\"feat_siRNA_{name}_GC_ratio_3\"] = (GC_ratio_3 == 0.52)\n",
    "\n",
    "    return df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publication_id = pd.get_dummies(df.publication_id)\n",
    "df_publication_id.columns = [\n",
    "    f\"feat_publication_id_{c}\" for c in df_publication_id.columns\n",
    "]\n",
    "df_gene_target_symbol_name = pd.get_dummies(df.gene_target_symbol_name)\n",
    "df_gene_target_symbol_name.columns = [\n",
    "    f\"feat_gene_target_symbol_name_{c}\" for c in df_gene_target_symbol_name.columns\n",
    "]\n",
    "df_gene_target_ncbi_id = pd.get_dummies(df.gene_target_ncbi_id)\n",
    "df_gene_target_ncbi_id.columns = [\n",
    "    f\"feat_gene_target_ncbi_id_{c}\" for c in df_gene_target_ncbi_id.columns\n",
    "]\n",
    "df_gene_target_species = pd.get_dummies(df.gene_target_species)\n",
    "df_gene_target_species.columns = [\n",
    "    f\"feat_gene_target_species_{c}\" for c in df_gene_target_species.columns\n",
    "]\n",
    "siRNA_duplex_id_values = df.siRNA_duplex_id.str.split(\"-|\\.\").str[1].astype(\"int\")\n",
    "siRNA_duplex_id_values = (siRNA_duplex_id_values - siRNA_duplex_id_values.min()) / (\n",
    "    siRNA_duplex_id_values.max() - siRNA_duplex_id_values.min()\n",
    ")\n",
    "df_siRNA_duplex_id = pd.DataFrame(siRNA_duplex_id_values)\n",
    "df_cell_line_donor = pd.get_dummies(df.cell_line_donor)\n",
    "df_cell_line_donor.columns = [\n",
    "    f\"feat_cell_line_donor_{c}\" for c in df_cell_line_donor.columns\n",
    "]\n",
    "df_cell_line_donor[\"feat_cell_line_donor_hepatocytes\"] = (\n",
    "    (df.cell_line_donor.str.contains(\"Hepatocytes\")).fillna(False).astype(\"int\")\n",
    ")\n",
    "df_cell_line_donor[\"feat_cell_line_donor_cells\"] = (\n",
    "    df.cell_line_donor.str.contains(\"Cells\").fillna(False).astype(\"int\")\n",
    ")\n",
    "df_siRNA_concentration = df.siRNA_concentration.to_frame()\n",
    "df_Transfection_method = pd.get_dummies(df.Transfection_method)\n",
    "df_Transfection_method.columns = [\n",
    "    f\"feat_Transfection_method_{c}\" for c in df_Transfection_method.columns\n",
    "]\n",
    "df_Duration_after_transfection_h = pd.get_dummies(df.Duration_after_transfection_h)\n",
    "df_Duration_after_transfection_h.columns = [\n",
    "    f\"feat_Duration_after_transfection_h_{c}\"\n",
    "    for c in df_Duration_after_transfection_h.columns\n",
    "]\n",
    "\n",
    "df_GRU_pred = df[['GRU_predict']]\n",
    "df_pretrained_pred = df[['Pretrained_feature_predict']]\n",
    "\n",
    "\n",
    "feats = pd.concat(\n",
    "    [\n",
    "        df_publication_id,\n",
    "        df_gene_target_symbol_name,\n",
    "        df_gene_target_ncbi_id,\n",
    "        df_gene_target_species,\n",
    "        df_siRNA_duplex_id,\n",
    "        df_cell_line_donor,\n",
    "        df_siRNA_concentration,\n",
    "        df_Transfection_method,\n",
    "        df_Duration_after_transfection_h,\n",
    "        siRNA_feat_builder(df.siRNA_sense_seq, False),\n",
    "        siRNA_feat_builder(df.siRNA_antisense_seq, True),\n",
    "        df_GRU_pred,\n",
    "        df_pretrained_pred,\n",
    "        df.iloc[:, -1].to_frame(),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "features = feats.iloc[:n_original, :-1]\n",
    "targets = feats.iloc[:n_original, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    targets,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# calculate_validation_score for GridSearchCV\n",
    "def calculate_validation_score(y_true, y_pred, threshold=30):\n",
    "    # y_pred = preds\n",
    "    # y_true = data.get_label()\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    # if mae < 0: mae = 0\n",
    "    # elif mae >100: mae = 100\n",
    "\n",
    "    y_true_binary = ((y_true <= threshold) & (y_true >= 0)).astype(int)\n",
    "    y_pred_binary = ((y_pred <= threshold) & (y_pred >= 0)).astype(int)\n",
    "\n",
    "    mask = (y_pred >= 0) & (y_pred <= threshold)\n",
    "    range_mae = (\n",
    "        mean_absolute_error(y_true[mask], y_pred[mask]) if np.sum(mask) > 0 else 100\n",
    "    )\n",
    "    # if range_mae < 0: range_mae = 0\n",
    "    # elif range_mae >100: range_mae = 100\n",
    "\n",
    "    # precision = precision_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "    # recall = recall_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "\n",
    "    if np.sum(y_pred_binary) > 0:\n",
    "        precision = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_pred_binary)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if np.sum(y_true_binary) > 0:\n",
    "        recall = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_true_binary)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    score = (1 - mae / 100) * 0.5 + (1 - range_mae / 100) * f1 * 0.5\n",
    "    return score\n",
    "\n",
    "custom_scorer = make_scorer(calculate_validation_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_metrics for lightgbm training\n",
    "def calculate_validation_score_for_training(preds, data, threshold=30):\n",
    "    y_pred = preds\n",
    "    y_true = data.get_label()\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    # if mae < 0: mae = 0\n",
    "    # elif mae >100: mae = 100\n",
    "\n",
    "    y_true_binary = ((y_true <= threshold) & (y_true >= 0)).astype(int)\n",
    "    y_pred_binary = ((y_pred <= threshold) & (y_pred >= 0)).astype(int)\n",
    "\n",
    "    mask = (y_pred >= 0) & (y_pred <= threshold)\n",
    "    range_mae = (\n",
    "        mean_absolute_error(y_true[mask], y_pred[mask]) if np.sum(mask) > 0 else 100\n",
    "    )\n",
    "    # if range_mae < 0: range_mae = 0\n",
    "    # elif range_mae >100: range_mae = 100\n",
    "\n",
    "    # precision = precision_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "    # recall = recall_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "\n",
    "    if np.sum(y_pred_binary) > 0:\n",
    "        precision = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_pred_binary)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if np.sum(y_true_binary) > 0:\n",
    "        recall = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_true_binary)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    score = (1 - mae / 100) * 0.5 + (1 - range_mae / 100) * f1 * 0.5\n",
    "    return \"custom_score\", score, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# # For the training data, double the size of the observations with y < 30:\n",
    "# def Below_30_double(X, y):\n",
    "#     idx = y < 30\n",
    "#     y = pd.concat([y, y[idx]])\n",
    "#     X = pd.concat([X, X[idx]])\n",
    "#     return X,y\n",
    "# X_train, y_train = Below_30_double(X_train, y_train)\n",
    "#######\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "\n",
    "# 定义一个回调函数来打印验证集的结果\n",
    "def print_validation_result(env):\n",
    "    result = env.evaluation_result_list[-1]\n",
    "    print(f\"[{env.iteration}] {result[1]}'s {result[0]}: {result[2]}\")\n",
    "\n",
    "'''\n",
    "# Grid Search for Hyperparameter Tuning (Optional)\n",
    "param_grid = {\n",
    "    'max_depth': [7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.02],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_fraction': [0.8, 0.9],\n",
    "    'bagging_freq': [0, 5, 10],\n",
    "    'n_estimators': [15000, 20000],\n",
    "    'min_child_samples': [20, 30, 50],\n",
    "}\n",
    "\n",
    "gbm = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression')\n",
    "grid = GridSearchCV(gbm, param_grid, cv=3, scoring=custom_scorer, verbose=1)\n",
    "grid.fit(features, targets)\n",
    "\n",
    "print(f'Best parameters found by grid search are: {grid.best_params_}')\n",
    "print(f'Best estimator found by grid search are: {grid.best_estimator_}')\n",
    "\n",
    "# Train with best parameters\n",
    "best_params = grid.best_params_\n",
    "best_estimator = grid.best_estimator_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved best parameters to improve testing efficiency\n",
    "best_params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"None\",\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"max_depth\": 9,\n",
    "    \"num_leaves\": 127,\n",
    "    \"min_child_samples\": 20\n",
    "}\n",
    "best_estimator = {\n",
    "    \"n_estimators\": 15000,\n",
    "}\n",
    "\n",
    "cv_gbm = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=best_estimator[\"n_estimators\"],\n",
    "    feval=calculate_validation_score_for_training,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    callbacks=[print_validation_result],\n",
    ")\n",
    "\n",
    "# Final Evaluation\n",
    "y_pred_best = cv_gbm.predict(X_test, num_iteration=cv_gbm.best_iteration)\n",
    "score_best = calculate_validation_score(y_test, y_pred_best)\n",
    "print(f'Validation score after tuning: {score_best}')\n",
    "\n",
    "# best score is 0.8288\n",
    "# Validation score after tuning: 0.8243341724610049\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = lgb.Dataset(features, label=targets)\n",
    "\n",
    "best_gbm = lgb.train(\n",
    "    best_params,\n",
    "    all_data,\n",
    "    num_boost_round=8700\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = feats.iloc[n_original:, :-1]\n",
    "y_pred = best_gbm.predict(eval_data)\n",
    "y_pred[y_pred>100] = 100\n",
    "y_pred[y_pred<0] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit[\"mRNA_remaining_pct\"] = y_pred\n",
    "df_submit.to_csv(\"../submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
