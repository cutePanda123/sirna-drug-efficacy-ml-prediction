{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abf300-2086-46b3-b480-5e1329d8513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from rich import print\n",
    "from sklearn.metrics import precision_score, recall_score, mean_absolute_error\n",
    "\n",
    "\n",
    "class GenomicTokenizer:\n",
    "    def __init__(self, ngram=3, stride=1):\n",
    "        self.ngram = ngram\n",
    "        self.stride = stride\n",
    "    def tokenize(self, t):\n",
    "        t = t.upper()\n",
    "        if self.ngram == 1:\n",
    "            toks = list(t)\n",
    "        else:\n",
    "            toks = [t[i:i+self.ngram] for i in range(0, len(t), self.stride) if len(t[i:i+self.ngram]) == self.ngram]\n",
    "        if len(toks[-1]) < self.ngram:\n",
    "            toks = toks[:-1]\n",
    "        return toks\n",
    "\n",
    "\n",
    "class GenomicVocab:\n",
    "    def __init__(self, itos):\n",
    "        self.itos = itos\n",
    "        self.stoi = {v:k for k,v in enumerate(self.itos)}\n",
    "        \n",
    "    @classmethod\n",
    "    def create(cls, tokens, max_vocab, min_freq):\n",
    "        freq = Counter(tokens)\n",
    "        itos = ['<pad>', '<cls>'] + [o for o,c in freq.most_common(max_vocab-1) if c >= min_freq]\n",
    "        return cls(itos)\n",
    "\n",
    "\n",
    "class SiRNADataset(Dataset):\n",
    "    def __init__(self, df, columns, vocab, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.columns = columns\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.bert_tokenizer = bert_tokenizer\n",
    "        self.max_length = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        ########################\n",
    "        # Tokenize and encode\n",
    "        ########################\n",
    "        # ['siRNA_antisense_seq', 'modified_siRNA_antisense_seq_list']\n",
    "        # seq0 = [str(row['gene_target_symbol_name'])]\n",
    "        seq1 = ['<cls>'] + self.tokenizer.tokenize(row['siRNA_antisense_seq'])\n",
    "        seq2 = ['<cls>'] + row['modified_siRNA_antisense_seq_list'].split()\n",
    "        \n",
    "        seq1 = seq1[:max_len] + ['<pad>']*(max_len-len(seq1))\n",
    "        seq2 = seq2[:max_len] + ['<pad>']*(max_len-len(seq2))\n",
    "        \n",
    "        # print(seq1+seq2+seq3)\n",
    "        encoded = [\n",
    "            torch.tensor(\n",
    "                [self.vocab.stoi.get(token, 0) for token in seq1], # Use 0 (pad) for unknown tokens\n",
    "                dtype=torch.int\n",
    "            ),\n",
    "            torch.tensor(\n",
    "                [self.vocab.stoi.get(token, 0) for token in seq2], # Use 0 (pad) for unknown tokens\n",
    "                dtype=torch.int\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        target = torch.tensor(row['mRNA_remaining_pct'], dtype=torch.float)\n",
    "        \n",
    "        return(encoded, target)\n",
    "\n",
    "\n",
    "class SiRNATransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_tokens: int,\n",
    "        d_embed: int=200,\n",
    "        n_heads: int=4,\n",
    "        d_feedforward: int=256,\n",
    "        n_layers: int=3,\n",
    "        dropout: float=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.embedding = nn.Embedding(n_tokens, d_embed, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    ##########################\n",
    "    # Model 2: Transformer Encoder\n",
    "    ##########################\n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=d_embed, nhead=n_heads, dim_feedforward=d_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=n_layers)\n",
    "        self.linear = nn.Linear(d_embed*2, 1)\n",
    "    def forward(self, input):\n",
    "        embedded = [self.embedding(seq) for seq in input]\n",
    "        outputs = []\n",
    "        for embed in embedded:\n",
    "            x = self.transformer_encoder(embed)\n",
    "            # x = self.dropout(x.mean(dim=1)) # mean pooling\n",
    "            x = self.dropout(x[:, 0, :])  # Use the first hidden state\n",
    "            outputs.append(x)\n",
    "        x = torch.cat(outputs, dim=1)\n",
    "        x = self.linear(x)\n",
    "        return(x.squeeze())\n",
    "    ##########################\n",
    "    \n",
    "    ##########################\n",
    "    # Model 1: GRU\n",
    "    ##########################\n",
    "    #     self.gru = nn.GRU(d_embed, d_feedforward, n_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "    #     self.linear = nn.Linear(d_feedforward*4, 1)\n",
    "    # def forward(self, input):\n",
    "    #     embedded = [self.embedding(seq) for seq in input]\n",
    "    #     outputs = []\n",
    "    #     for embed in embedded:\n",
    "    #         x, _ = self.gru(embed)\n",
    "    #         x = self.dropout(x[:, -1, :])  # Use last hidden state\n",
    "    #         outputs.append(x)\n",
    "    #     x = torch.cat(outputs, dim=1)\n",
    "    #     x = self.linear(x)\n",
    "    #     return(x.squeeze())\n",
    "    ##########################\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, threshold=30):\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    y_true_binary = (y_true < threshold).astype(int)\n",
    "    y_pred_binary = (y_pred < threshold).astype(int)\n",
    "\n",
    "    mask = (y_pred >= 0) & (y_pred <= threshold)\n",
    "    range_mae = mean_absolute_error(y_true[mask], y_pred[mask]) if mask.sum() > 0 else 100\n",
    "    \n",
    "    precision = precision_score(y_true_binary, y_pred_binary, average='binary')\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, average='binary')\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    score = (1 - mae / 100) * 0.5 + (1 - range_mae / 100) * f1 * 0.5\n",
    "    return score\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=50, device='cuda'):\n",
    "    model.to(device)\n",
    "    best_score = -float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            x = [temp.to(device) for temp in x]\n",
    "            y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        val_preds = np.array(val_preds)\n",
    "        val_targets = np.array(val_targets)\n",
    "        score = calculate_metrics(val_targets, val_preds)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print(f'Validation Score: {score:.4f}')\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model.state_dict().copy()\n",
    "            print(f'New best model found with score: {best_score:.4f}')\n",
    "        if not scheduler is None:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25050fbb-d399-4229-b8d0-d29cc2a9cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "columns = ['siRNA_antisense_seq', 'modified_siRNA_antisense_seq_list']\n",
    "train_data.dropna(subset=columns + ['mRNA_remaining_pct'], inplace=True)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create vocabulary\n",
    "tokenizer = GenomicTokenizer(ngram=3, stride=1)\n",
    "\n",
    "all_tokens = []\n",
    "for seq in train_data['siRNA_antisense_seq']:\n",
    "    all_tokens.extend(tokenizer.tokenize(seq))\n",
    "for seq in train_data['modified_siRNA_antisense_seq_list']:\n",
    "    all_tokens.extend(seq.split())\n",
    "# for seq in train_data['gene_target_symbol_name']:\n",
    "#     all_tokens.append(seq)\n",
    "\n",
    "vocab = GenomicVocab.create(all_tokens, max_vocab=10000, min_freq=1)\n",
    "print('Vocabulary Size: '+str(len(vocab.itos)))\n",
    "# Find max sequence length\n",
    "max_len = max(max(len(seq.split()) if ' ' in seq else len(tokenizer.tokenize(seq)) \n",
    "                  for seq in train_data[col]) for col in columns)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SiRNADataset(train_data, columns, vocab, tokenizer, max_len)\n",
    "val_dataset = SiRNADataset(val_data, columns, vocab, tokenizer, max_len)\n",
    "test_dataset = SiRNADataset(test_data, columns, vocab, tokenizer, max_len)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539424cd-81a7-42f8-b217-306f29ecb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = SiRNATransformerModel(\n",
    "    n_tokens=len(vocab.itos),\n",
    "    d_embed=200,\n",
    "    n_heads=2,\n",
    "    d_feedforward=256,\n",
    "    n_layers=3,\n",
    "    dropout=0.5\n",
    ")\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d599b7-4163-4aee-bdc6-5d86aa939cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optim_scheduler = None\n",
    "# optim_scheduler = optim.lr_scheduler.StepLR(model_optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "training_epochs = 300\n",
    "train_model(model, train_loader, val_loader, criterion, model_optimizer, optim_scheduler, training_epochs, device)\n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6665215-443c-43c3-a368-3f57701bf9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843ac24-b42d-4b2c-b179-431231011c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85584eb0-e480-4d1c-bf7b-4a99e7fff83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb71d62-02d1-46e4-84af-82c085dd0029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f45cdb-dace-4a24-bc36-48d85d2aae4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c8ec8-358c-470e-a63c-7c7ca97c3a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b82419a-3434-469c-8bb2-ec31d1447ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "predictions = []\n",
    "for test_inputs, targets in tqdm(test_loader):\n",
    "    inputs = [x.to(device) for x in test_inputs]\n",
    "    outputs = model(inputs)\n",
    "    predictions.extend(outputs.detach().cpu().numpy())\n",
    "print(\"Finished get the prediction output.\")\n",
    "\n",
    "input_file = 'sample_submission.csv'\n",
    "output_file = 'processed_submission.csv'\n",
    "\n",
    "# Check if the output file exists and remove it if it does\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "with open(input_file, mode='r') as infile, open(output_file, mode='w', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for i, row in enumerate(reader):\n",
    "        row['mRNA_remaining_pct'] = predictions[i]\n",
    "        writer.writerow(row)\n",
    "print(\"Finished save outputs to result file(processed_submission.csv).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388955b1-f25e-453c-adf5-1d5d84715c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
