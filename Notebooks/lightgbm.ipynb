{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"../train_data.csv\")\n",
    "n_original = df_original.shape[0]\n",
    "df_submit = pd.read_csv(\"../sample_submission.csv\")\n",
    "df = pd.concat([df_original, df_submit], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Features from GRU\n",
    "df = pd.read_csv(\"../GRU_features_predict_only.csv\").merge(\n",
    "    df,\n",
    "    on='id'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siRNA_feat_builder(s: pd.Series, anti: bool = False):\n",
    "    name = \"anti\" if anti else \"sense\"\n",
    "    df = s.to_frame()\n",
    "    df[f\"feat_siRNA_{name}_seq_len\"] = s.str.len()\n",
    "    for pos in [0, -1]:\n",
    "        for c in list(\"AUGC\"):\n",
    "            df[f\"feat_siRNA_{name}_seq_{c}_{'front' if pos == 0 else 'back'}\"] = (\n",
    "                s.str[pos] == c\n",
    "            )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_1\"] = s.str.startswith(\"AA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_2\"] = s.str.startswith(\"GA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_3\"] = s.str.startswith(\"CA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_4\"] = s.str.startswith(\"UA\") & s.str.endswith(\n",
    "        \"UU\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_5\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"AA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_6\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"GA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_7\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"CA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_8\"] = s.str.startswith(\"UU\") & s.str.endswith(\n",
    "        \"UA\"\n",
    "    )\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_9\"] = s.str[1] == \"A\"\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_10\"] = s.str[-2] == \"A\"\n",
    "    df[f\"feat_siRNA_{name}_seq_pattern_GC_ratio_0\"] = (\n",
    "        s.str.count(\"G\") + s.str.count(\"C\")\n",
    "    ) / s.str.len()\n",
    "\n",
    "    df[f\"feat_siRNA_{name}_len_range\"] = (s.str.len() >= 21) & (s.str.len() <= 25)\n",
    "\n",
    "    GC_ratio_1 = (s.str.count(\"G\") + s.str.count(\"C\")) / s.str.len()\n",
    "    df[f\"feat_siRNA_{name}_GC_ratio_1\"] = (GC_ratio_1 >= 0.31) & (GC_ratio_1 <= 0.58)\n",
    "\n",
    "    GC_ratio_2 = (s.str[1:7].str.count(\"G\") + s.str[1:7].str.count(\"C\")) / s.str[1:7].str.len()\n",
    "    df[f\"feat_siRNA_{name}_GC_ratio_2\"] = (GC_ratio_2 == 0.19)\n",
    "\n",
    "    GC_ratio_3 = (s.str[7:18].str.count(\"G\") + s.str[7:18].str.count(\"C\")) / s.str[7:18].str.len()\n",
    "    df[f\"feat_siRNA_{name}_GC_ratio_3\"] = (GC_ratio_3 == 0.52)\n",
    "\n",
    "    return df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_publication_id = pd.get_dummies(df.publication_id)\n",
    "df_publication_id.columns = [\n",
    "    f\"feat_publication_id_{c}\" for c in df_publication_id.columns\n",
    "]\n",
    "df_gene_target_symbol_name = pd.get_dummies(df.gene_target_symbol_name)\n",
    "df_gene_target_symbol_name.columns = [\n",
    "    f\"feat_gene_target_symbol_name_{c}\" for c in df_gene_target_symbol_name.columns\n",
    "]\n",
    "df_gene_target_ncbi_id = pd.get_dummies(df.gene_target_ncbi_id)\n",
    "df_gene_target_ncbi_id.columns = [\n",
    "    f\"feat_gene_target_ncbi_id_{c}\" for c in df_gene_target_ncbi_id.columns\n",
    "]\n",
    "df_gene_target_species = pd.get_dummies(df.gene_target_species)\n",
    "df_gene_target_species.columns = [\n",
    "    f\"feat_gene_target_species_{c}\" for c in df_gene_target_species.columns\n",
    "]\n",
    "siRNA_duplex_id_values = df.siRNA_duplex_id.str.split(\"-|\\.\").str[1].astype(\"int\")\n",
    "siRNA_duplex_id_values = (siRNA_duplex_id_values - siRNA_duplex_id_values.min()) / (\n",
    "    siRNA_duplex_id_values.max() - siRNA_duplex_id_values.min()\n",
    ")\n",
    "df_siRNA_duplex_id = pd.DataFrame(siRNA_duplex_id_values)\n",
    "df_cell_line_donor = pd.get_dummies(df.cell_line_donor)\n",
    "df_cell_line_donor.columns = [\n",
    "    f\"feat_cell_line_donor_{c}\" for c in df_cell_line_donor.columns\n",
    "]\n",
    "df_cell_line_donor[\"feat_cell_line_donor_hepatocytes\"] = (\n",
    "    (df.cell_line_donor.str.contains(\"Hepatocytes\")).fillna(False).astype(\"int\")\n",
    ")\n",
    "df_cell_line_donor[\"feat_cell_line_donor_cells\"] = (\n",
    "    df.cell_line_donor.str.contains(\"Cells\").fillna(False).astype(\"int\")\n",
    ")\n",
    "df_siRNA_concentration = df.siRNA_concentration.to_frame()\n",
    "df_Transfection_method = pd.get_dummies(df.Transfection_method)\n",
    "df_Transfection_method.columns = [\n",
    "    f\"feat_Transfection_method_{c}\" for c in df_Transfection_method.columns\n",
    "]\n",
    "df_Duration_after_transfection_h = pd.get_dummies(df.Duration_after_transfection_h)\n",
    "df_Duration_after_transfection_h.columns = [\n",
    "    f\"feat_Duration_after_transfection_h_{c}\"\n",
    "    for c in df_Duration_after_transfection_h.columns\n",
    "]\n",
    "\n",
    "df_GRU_pred = df[['GRU_predict']]\n",
    "\n",
    "'''\n",
    "# More GRU realted features for experimentation\n",
    "df_GRU_feature = df[ df.columns[['GRU_feature_' in c for c in df.columns]] ]\n",
    "important_GRU_features = [\n",
    "    'GRU_feature_11','GRU_feature_18','GRU_feature_523','GRU_feature_679','GRU_feature_689'\n",
    "]\n",
    "df_GRU_feature = df_GRU_feature[important_GRU_features]\n",
    "'''\n",
    "\n",
    "feats = pd.concat(\n",
    "    [\n",
    "        df_publication_id,\n",
    "        df_gene_target_symbol_name,\n",
    "        df_gene_target_ncbi_id,\n",
    "        df_gene_target_species,\n",
    "        df_siRNA_duplex_id,\n",
    "        df_cell_line_donor,\n",
    "        df_siRNA_concentration,\n",
    "        df_Transfection_method,\n",
    "        df_Duration_after_transfection_h,\n",
    "        siRNA_feat_builder(df.siRNA_sense_seq, False),\n",
    "        siRNA_feat_builder(df.siRNA_antisense_seq, True),\n",
    "        df_GRU_pred,\n",
    "        # df_GRU_feature,\n",
    "        df.iloc[:, -1].to_frame(),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30656, 212)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, callback as xgb_callback\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "features = feats.iloc[:n_original, :-1]\n",
    "targets = feats.iloc[:n_original, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features,\n",
    "    targets,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "# calculate_validation_score for GridSearchCV\n",
    "def calculate_validation_score(y_true, y_pred, threshold=30):\n",
    "    # y_pred = preds\n",
    "    # y_true = data.get_label()\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    # if mae < 0: mae = 0\n",
    "    # elif mae >100: mae = 100\n",
    "\n",
    "    y_true_binary = ((y_true <= threshold) & (y_true >= 0)).astype(int)\n",
    "    y_pred_binary = ((y_pred <= threshold) & (y_pred >= 0)).astype(int)\n",
    "\n",
    "    mask = (y_pred >= 0) & (y_pred <= threshold)\n",
    "    range_mae = (\n",
    "        mean_absolute_error(y_true[mask], y_pred[mask]) if np.sum(mask) > 0 else 100\n",
    "    )\n",
    "    # if range_mae < 0: range_mae = 0\n",
    "    # elif range_mae >100: range_mae = 100\n",
    "\n",
    "    # precision = precision_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "    # recall = recall_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "\n",
    "    if np.sum(y_pred_binary) > 0:\n",
    "        precision = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_pred_binary)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if np.sum(y_true_binary) > 0:\n",
    "        recall = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_true_binary)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    score = (1 - mae / 100) * 0.5 + (1 - range_mae / 100) * f1 * 0.5\n",
    "    return score\n",
    "\n",
    "custom_scorer = make_scorer(calculate_validation_score, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_metrics for lightgbm training\n",
    "def calculate_validation_score_for_training(preds, data, threshold=30):\n",
    "    y_pred = preds\n",
    "    y_true = data.get_label()\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    # if mae < 0: mae = 0\n",
    "    # elif mae >100: mae = 100\n",
    "\n",
    "    y_true_binary = ((y_true <= threshold) & (y_true >= 0)).astype(int)\n",
    "    y_pred_binary = ((y_pred <= threshold) & (y_pred >= 0)).astype(int)\n",
    "\n",
    "    mask = (y_pred >= 0) & (y_pred <= threshold)\n",
    "    range_mae = (\n",
    "        mean_absolute_error(y_true[mask], y_pred[mask]) if np.sum(mask) > 0 else 100\n",
    "    )\n",
    "    # if range_mae < 0: range_mae = 0\n",
    "    # elif range_mae >100: range_mae = 100\n",
    "\n",
    "    # precision = precision_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "    # recall = recall_score(y_true_binary, y_pred_binary, average=\"binary\")\n",
    "\n",
    "    if np.sum(y_pred_binary) > 0:\n",
    "        precision = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_pred_binary)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if np.sum(y_true_binary) > 0:\n",
    "        recall = (np.array(y_pred_binary) & y_true_binary).sum()/np.sum(y_true_binary)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    score = (1 - mae / 100) * 0.5 + (1 - range_mae / 100) * f1 * 0.5\n",
    "    return \"custom_score\", score, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# For the training data, double the size of the observations with y < 30:\n",
    "def Get_sample_weight(y):\n",
    "    weights = list(map(lambda x: 2 if (x<=30 and x>=0) else 1, y))\n",
    "    return np.array(weights)\n",
    "#######\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train, weight=Get_sample_weight(y_train))\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data, weight=Get_sample_weight(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Grid Search for Hyperparameter Tuning (Optional)\\nparam_grid = {\\n    \\'max_depth\\': [7, 9, 11],\\n    \\'learning_rate\\': [0.01, 0.02],\\n    \\'num_leaves\\': [31, 63, 127],\\n    \\'feature_fraction\\': [0.8, 0.9],\\n    \\'bagging_fraction\\': [0.8, 0.9],\\n    \\'bagging_freq\\': [0, 5, 10],\\n    \\'n_estimators\\': [8700, 15000, 20000],\\n    \\'min_child_samples\\': [20, 30, 50],\\n}\\n\\nif loss_method == \"RMSE\"\\n    gbm = lgb.LGBMRegressor(boosting_type=\\'gbdt\\', objective=\\'regression\\', metric=\\'rmse\\', n_estimators=15000)\\n    grid = GridSearchCV(gbm, param_grid, cv=3, scoring=\\'neg_root_mean_squared_error\\', verbose=1)\\nelse:\\n    gbm = lgb.LGBMRegressor(boosting_type=\\'gbdt\\', objective=\\'regression\\')\\n    grid = GridSearchCV(gbm, param_grid, cv=3, scoring=custom_scorer, verbose=1)\\n\\ngrid.fit(features, targets)\\n\\nprint(f\\'Best parameters found by grid search are: {grid.best_params_}\\')\\nprint(f\\'Best estimator found by grid search are: {grid.best_estimator_}\\')\\n\\n# Train with best parameters\\nbest_params = grid.best_params_\\nbest_estimator = grid.best_estimator_\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# change loss_method to \"custom\" if you want to use same method recommended by the competition \n",
    "loss_method = \"RMSE\"\n",
    "using_stacking = True\n",
    "\n",
    "# 定义一个回调函数来打印验证集的结果\n",
    "def print_validation_result(env):\n",
    "    result = env.evaluation_result_list[-1]\n",
    "    print(f\"[{env.iteration}] {result[1]}'s {result[0]}: {result[2]}\")\n",
    "\n",
    "'''\n",
    "# Grid Search for Hyperparameter Tuning (Optional)\n",
    "param_grid = {\n",
    "    'max_depth': [7, 9, 11],\n",
    "    'learning_rate': [0.01, 0.02],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'feature_fraction': [0.8, 0.9],\n",
    "    'bagging_fraction': [0.8, 0.9],\n",
    "    'bagging_freq': [0, 5, 10],\n",
    "    'n_estimators': [8700, 15000, 20000],\n",
    "    'min_child_samples': [20, 30, 50],\n",
    "}\n",
    "\n",
    "if loss_method == \"RMSE\"\n",
    "    gbm = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression', metric='rmse', n_estimators=15000)\n",
    "    grid = GridSearchCV(gbm, param_grid, cv=3, scoring='neg_root_mean_squared_error', verbose=1)\n",
    "else:\n",
    "    gbm = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression')\n",
    "    grid = GridSearchCV(gbm, param_grid, cv=3, scoring=custom_scorer, verbose=1)\n",
    "\n",
    "grid.fit(features, targets)\n",
    "\n",
    "print(f'Best parameters found by grid search are: {grid.best_params_}')\n",
    "print(f'Best estimator found by grid search are: {grid.best_estimator_}')\n",
    "\n",
    "# Train with best parameters\n",
    "best_params = grid.best_params_\n",
    "best_estimator = grid.best_estimator_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# xgb_lr_array\n",
    "xgb_lr = []\n",
    "# lgb_lr_array\n",
    "lgb_lr = []\n",
    "\n",
    "def adaptive_learning_rate(decay_rate=0.8, patience=50):\n",
    "    best_score = float(\"inf\")  # initialize with positive infinity since a lower score is better.\n",
    "    wait = 0\n",
    "\n",
    "    def callback(env):\n",
    "        nonlocal best_score, wait\n",
    "        # Find the score in evals_log[-1][2]\n",
    "        current_score = env.evaluation_result_list[-1][2]\n",
    "        current_lr =  env.model.params.get('learning_rate')\n",
    "\n",
    "        # TODO: save learning rate \n",
    "        # lgb_lr.append(current_lr)\n",
    "\n",
    "        if current_score < best_score: \n",
    "            best_score = current_score\n",
    "            # wait = 0 # the score needs to have no continuous improvement.\n",
    "        else:\n",
    "            wait += 1\n",
    "\n",
    "        if wait >= patience:\n",
    "            new_lr = float(current_lr) * decay_rate\n",
    "            wait = 0\n",
    "            env.model.params['learning_rate'] = new_lr\n",
    "            print(f\"Learning rate adjusted to {env.model.params.get('learning_rate')}\")\n",
    "\n",
    "    return callback\n",
    "\n",
    "def custom_rmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return 'custom_rmse', rmse, False\n",
    "\n",
    "def custom_rmse_for_xgb(y_true, y_pred):\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    # Return the metric name, value, and whether higher values are better (False for RMSE)\n",
    "    return 'custom_rmse', rmse\n",
    "\n",
    "\n",
    "class AdaptiveLearningRateCallback(xgb_callback.TrainingCallback):\n",
    "    def __init__(self, decay_rate=0.8, patience=50, initial_lr=0.1):\n",
    "        self.decay_rate = decay_rate\n",
    "        self.patience = patience\n",
    "        self.best_score = float('inf')\n",
    "        self.wait = 0\n",
    "        self.current_lr = initial_lr\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        if 'validation_0' in evals_log and 'rmse' in evals_log['validation_0']:\n",
    "            current_score = evals_log['validation_0']['rmse'][-1]\n",
    "            if current_score < self.best_score:\n",
    "                self.best_score = current_score\n",
    "                self.wait = 0\n",
    "            else:\n",
    "                self.wait += 1\n",
    "\n",
    "            # TODO: save learning rate \n",
    "            # xgb_lr.append(self.current_lr)\n",
    "\n",
    "            if self.wait >= self.patience:\n",
    "                self.current_lr *= self.decay_rate\n",
    "                model.set_param('learning_rate', self.current_lr)\n",
    "                self.wait = 0\n",
    "                print(f\"Learning rate adjusted to {self.current_lr}\")\n",
    "        else:\n",
    "            # Debugging: print available keys\n",
    "            print(\"Available keys in evals_log:\", evals_log.keys())\n",
    "\n",
    "        return False  # Continue training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved best parameters to improve testing efficiency\n",
    "best_estimator = {\n",
    "    \"n_estimators\": 15000,\n",
    "}\n",
    "\n",
    "# Custom callback to apply adaptive learning rate for LightGbm\n",
    "adaptive_lr = adaptive_learning_rate(decay_rate=0.9, patience=1000)\n",
    "\n",
    "# Adaptive learning rate for XGBoost using a learning rate scheduler and training data\n",
    "adaptive_lr_callback = AdaptiveLearningRateCallback(decay_rate=0.8, patience=50)\n",
    "\n",
    "# LightGBM model with custom learning rate callback\n",
    "lgb_reg = lgb.LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    objective='regression',\n",
    "    min_child_samples=20,\n",
    "    num_leaves=127,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.02,  # Initial learning rate\n",
    "    n_estimators=best_estimator[\"n_estimators\"]\n",
    ")\n",
    "\n",
    "# Fitting LightGBM model with custom callback and training data\n",
    "lgb_reg.fit(X_train, y_train,\n",
    "            sample_weight=Get_sample_weight(y_train),\n",
    "            callbacks=[\n",
    "                adaptive_lr,\n",
    "                lgb.early_stopping(stopping_rounds=int(best_estimator[\"n_estimators\"] * 0.1)),\n",
    "                print_validation_result\n",
    "            ],\n",
    "            eval_metric=custom_rmse,\n",
    "            eval_set=[(X_test, y_test)])\n",
    "\n",
    "# XGBoost model with learning rate scheduler\n",
    "xgb_reg = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=best_estimator[\"n_estimators\"],\n",
    "    callbacks=[adaptive_lr_callback],\n",
    "    eval_metric='rmse',\n",
    "    learning_rate=0.1,  # Initial learning rate\n",
    ")\n",
    "\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "            sample_weight=Get_sample_weight(y_train),\n",
    "            eval_set=[(X_test, y_test)])\n",
    "\n",
    "# Create meta-features from the predictions of the base estimators\n",
    "meta_features_train = np.column_stack([\n",
    "    lgb_reg.predict(X_train),\n",
    "    xgb_reg.predict(X_train)\n",
    "])\n",
    "\n",
    "meta_features_test = np.column_stack([\n",
    "    lgb_reg.predict(X_test),\n",
    "    xgb_reg.predict(X_test)\n",
    "])\n",
    "\n",
    "# Define and fit the final estimator\n",
    "stacking_estimator = LinearRegression()\n",
    "stacking_estimator.fit(meta_features_train, y_train)\n",
    "\n",
    "if loss_method == \"RMSE\":\n",
    "    best_params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"regression\",\n",
    "         \"metric\": \"root_mean_squared_error\",\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 9,\n",
    "        \"num_leaves\": 127,\n",
    "        \"min_child_samples\": 20\n",
    "    }\n",
    "\n",
    "    if using_stacking:\n",
    "        stacking_estimator.fit(meta_features_train, y_train)\n",
    "    else:\n",
    "        best_gbm = lgb.train(\n",
    "            best_params,\n",
    "            train_data,\n",
    "            num_boost_round=best_estimator[\"n_estimators\"],\n",
    "            valid_sets=[train_data, test_data],\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=int(best_estimator[\"n_estimators\"] * 0.1)),\n",
    "                print_validation_result\n",
    "            ],\n",
    "        )\n",
    "else:\n",
    "    best_params = {\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"None\",\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 9,\n",
    "        \"num_leaves\": 127,\n",
    "        \"min_child_samples\": 20\n",
    "    }\n",
    "    \n",
    "    best_gbm = lgb.train(\n",
    "        best_params,\n",
    "        train_data,\n",
    "        num_boost_round=best_estimator[\"n_estimators\"],\n",
    "        feval=calculate_validation_score_for_training,\n",
    "        valid_sets=[train_data, test_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=int(best_estimator[\"n_estimators\"] * 0.1)),\n",
    "            print_validation_result\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# Final Evaluation\n",
    "if using_stacking:\n",
    "    y_pred_eval = stacking_estimator.predict(meta_features_test)\n",
    "else:\n",
    "    y_pred_eval = best_gbm.predict(X_test, num_iteration=best_gbm.best_iteration)\n",
    "\n",
    "if loss_method == \"RMSE\":\n",
    "    rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_eval))\n",
    "    print(f'Validation RMSE after tuning: {rmse_best}')\n",
    "else:\n",
    "    score_best = calculate_validation_score(y_test, y_pred_eval)\n",
    "    print(f'Validation score after tuning: {score_best}')\n",
    "\n",
    "# best score is 0.8288\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finial train and predict\n",
    "\n",
    "if using_stacking:\n",
    "    best_model = stacking_estimator\n",
    "else:\n",
    "    all_data = lgb.Dataset(features, label=targets, weight=Get_sample_weight(targets))\n",
    "    best_model = lgb.train(\n",
    "        best_params,\n",
    "        all_data,\n",
    "        num_boost_round=best_estimator[\"n_estimators\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = feats.iloc[n_original:, :-1]\n",
    "\n",
    "if using_stacking:\n",
    "    meta_features_eval = np.column_stack([\n",
    "        lgb_reg.predict(eval_data),\n",
    "        xgb_reg.predict(eval_data)\n",
    "    ])\n",
    "    eval_data = meta_features_eval\n",
    "\n",
    "y_pred_best = best_model.predict(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit[\"mRNA_remaining_pct\"] = y_pred_best\n",
    "df_submit.to_csv(\"../submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
